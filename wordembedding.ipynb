{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eb066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import torch.optim as optim\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff94b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing chunks with NLTK...\n",
      "0 tokens processed\n",
      "1,000,000 tokens processed\n",
      "2,000,000 tokens processed\n",
      "3,000,000 tokens processed\n",
      "4,000,000 tokens processed\n",
      "5,000,000 tokens processed\n",
      "6,000,000 tokens processed\n",
      "7,000,000 tokens processed\n",
      "8,000,000 tokens processed\n",
      "9,000,000 tokens processed\n",
      "Tokenization complete!\n",
      "Unique words: 158483\n"
     ]
    }
   ],
   "source": [
    "# FOR NEWOUTPUT.TXT\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "max_bytes = 60 * 1024 * 1024\n",
    "\n",
    "with open('new_output.txt', 'rb') as f:\n",
    "    raw_bytes = f.read(max_bytes)\n",
    "\n",
    "text = raw_bytes.decode('utf-8', errors='ignore')\n",
    "\n",
    "chunk_size = 10_000_000  # About ~10MB per chunk\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def stream_nltk_tokens(text_chunks):\n",
    "    for chunk in text_chunks:\n",
    "        tokens = word_tokenize(chunk.lower(), preserve_line=True)\n",
    "        for token in tokens:\n",
    "            if token.isalpha():\n",
    "                yield token\n",
    "\n",
    "word_counts = Counter()\n",
    "print(\"Tokenizing chunks with NLTK...\")\n",
    "\n",
    "for i, token in enumerate(stream_nltk_tokens(chunks)):\n",
    "    word_counts[token] += 1\n",
    "    if i % 1_000_000 == 0:\n",
    "        print(f\"{i:,} tokens processed\")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Unique words: {len(word_counts)}\")\n",
    "\n",
    "min_freq = 5\n",
    "max_vocab_size = None\n",
    "\n",
    "def is_clean_word(word):\n",
    "    return re.fullmatch(r\"[a-z]+\", word) is not None\n",
    "\n",
    "cleaned_word_counts = {\n",
    "    word: freq for word, freq in word_counts.items()\n",
    "    if is_clean_word(word) and freq >= min_freq\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5419c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286501\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43bbb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned vocab size: 45700\n",
      "Sample vocab entries: [('are', 0), ('you', 1), ('a', 2), ('resident', 3), ('of', 4), ('pinnacle', 5), ('who', 6), ('owns', 7), ('small', 8), ('business', 9)]\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {word: idx for idx, word in enumerate(cleaned_word_counts)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "\n",
    "print(f\"Final cleaned vocab size: {len(word_to_idx)}\")\n",
    "print(\"Sample vocab entries:\", list(word_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a639b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def generate_skipgram_pairs_from_stream(token_stream, word_to_idx, window_size=2, chunk_size=100_000):\n",
    "    pairs_tensor_list = []\n",
    "    pairs = []\n",
    "    window = deque(maxlen=2 * window_size + 1)\n",
    "\n",
    "    for token in token_stream:\n",
    "        if token not in word_to_idx:\n",
    "            continue\n",
    "\n",
    "        window.append(token)\n",
    "\n",
    "        if len(window) < window.maxlen:\n",
    "            continue  # fill window first\n",
    "\n",
    "        center_pos = window_size\n",
    "        center_word = window[center_pos]\n",
    "\n",
    "        for i, context_word in enumerate(window):\n",
    "            if i == center_pos:\n",
    "                continue\n",
    "            if context_word in word_to_idx:\n",
    "                pairs.append([word_to_idx[center_word], word_to_idx[context_word]])\n",
    "\n",
    "        if len(pairs) >= chunk_size:\n",
    "            chunk_tensor = torch.tensor(pairs, dtype=torch.long)\n",
    "            pairs_tensor_list.append(chunk_tensor)\n",
    "            pairs = []\n",
    "            gc.collect()\n",
    "\n",
    "    # Final leftovers\n",
    "    if pairs:\n",
    "        chunk_tensor = torch.tensor(pairs, dtype=torch.long)\n",
    "        pairs_tensor_list.append(chunk_tensor)\n",
    "        del pairs\n",
    "        gc.collect()\n",
    "\n",
    "    return torch.cat(pairs_tensor_list, dim=0)\n",
    "\n",
    "pairs_tensor = generate_skipgram_pairs_from_stream(\n",
    "    token_stream=stream_nltk_tokens(chunks),\n",
    "    word_to_idx=word_to_idx,\n",
    "    window_size=2,\n",
    "    chunk_size=100_000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c028c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pairs_tensor, \"pairs_tensor_nltk_small.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2955954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_1676\\4047217220.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pairs_tensor = torch.load(\"pairs_tensor_nltk_small.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38117316, 2])\n",
      "tensor([[2, 0],\n",
      "        [2, 1],\n",
      "        [2, 3],\n",
      "        [2, 4],\n",
      "        [3, 1]])\n"
     ]
    }
   ],
   "source": [
    "pairs_tensor = torch.load(\"pairs_tensor_nltk_small.pt\")\n",
    "print(pairs_tensor.shape)\n",
    "print(pairs_tensor[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d7988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs_tensor):\n",
    "        self.pairs = pairs_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pairs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return center, context\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8310728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SkipGramDataset(pairs_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=0, pin_memory=True, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e56515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGramNegSampling, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        center_embeds = self.in_embed(center_words)\n",
    "        pos_embeds = self.out_embed(pos_context_words)\n",
    "        neg_embeds = self.out_embed(neg_context_words)\n",
    "\n",
    "        pos_score = torch.sum(center_embeds * pos_embeds, dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        neg_score = torch.bmm(neg_embeds.neg(), center_embeds.unsqueeze(2)).squeeze(2)\n",
    "        neg_loss = F.logsigmoid(neg_score).sum(1)\n",
    "\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "    \n",
    "def get_negative_samples(batch_size, vocab_size, num_neg_samples):\n",
    "    neg_samples = torch.randint(0, vocab_size, (batch_size, num_neg_samples))\n",
    "    return neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e790639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch 0\n",
      "Epoch 1 / 5 - Loss: 3.5666\n",
      "Embeddings checkpoint saved\n",
      "epoch 1\n",
      "Epoch 2 / 5 - Loss: 1.1926\n",
      "Embeddings checkpoint saved\n",
      "epoch 2\n",
      "Epoch 3 / 5 - Loss: 1.0575\n",
      "Embeddings checkpoint saved\n",
      "epoch 3\n",
      "Epoch 4 / 5 - Loss: 1.0186\n",
      "Embeddings checkpoint saved\n",
      "epoch 4\n",
      "Epoch 5 / 5 - Loss: 1.0006\n",
      "Embeddings checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_idx)\n",
    "embedding_dim = 256\n",
    "num_neg_samples = 5\n",
    "epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SkipGramNegSampling(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    print(f\"epoch {epoch}\")\n",
    "    for center, context in dataloader:\n",
    "        center = center.to(device, non_blocking=True)\n",
    "        context = context.to(device, non_blocking=True)\n",
    "\n",
    "        batch_size = center.shape[0]\n",
    "        negative_samples = get_negative_samples(batch_size, vocab_size, num_neg_samples).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center, context, negative_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch + 1} / {epochs} - Loss: {average_loss:.4f}')\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': average_loss,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'vocab_size': vocab_size\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, f\"checkpoint_epoch_nltk{epoch+1}.pt\")\n",
    "    print(\"Embeddings checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937877b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.device_count()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f82adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_25516\\1300141383.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint_epoch_3.pt', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 3\n",
      "epoch 3\n",
      "Epoch 4 / 5 - Loss: 0.8494\n",
      "Embeddings checkpoint saved\n",
      "epoch 4\n",
      "Epoch 5 / 5 - Loss: 0.8504\n",
      "Embeddings checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "# reload the checkpoint\n",
    "checkpoint = torch.load('checkpoint_epoch_3.pt', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "embedding_dim = checkpoint['embedding_dim']\n",
    "num_neg_samples = 5\n",
    "\n",
    "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "start_epoch = checkpoint['epoch'] + 1  # resume at the next epoch\n",
    "epochs = 5  \n",
    "\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    print(f\"epoch {epoch}\")\n",
    "    for center, context in dataloader:\n",
    "        center = center.to(device, non_blocking=True)\n",
    "        context = context.to(device, non_blocking=True)\n",
    "\n",
    "        batch_size = center.shape[0]\n",
    "        negative_samples = get_negative_samples(batch_size, vocab_size, num_neg_samples)\n",
    "        negative_samples = negative_samples.to(device, non_blocking=True)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center, context, negative_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch + 1} / {epochs} - Loss: {average_loss:.4f}')\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': average_loss,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'vocab_size': vocab_size\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    print(\"Embeddings checkpoint saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
